# Vocabulary Speedrunner — AI-based 高效率單字記憶引擎

> 以真實世界的詞頻為導航，以本地 AI 為家教，讓你的單字背誦像「Speedrun」一樣迅速、精準、且充滿智慧。

## 專案介紹

**Vocabulary Speedrunㄙer** 是一款專為高目標學習者設計的次世代單字記憶輔助工具。它摒棄了傳統單字 app 的死板規則，引入了兩大核心技術：**大規模語料庫詞頻分析**與**本地大型語言模型 (LLM) 語意判斷**。

本專案的核心是解決學習效率的根本問題：在有限的時間內，應該先學哪個單字？以及如何判斷一個單字「真正學會了」？我們的答案是，讓資料和 AI 來引導您。

## 核心功能

-   **基於真實語料庫的權重分配**：程式使用從**阿拉伯語維基百科**全量分析得出的詞頻資料庫 (`voc_database.csv`)，自動為您的備審單字表分配初始重要性權重，讓您優先學習高價值單字。

-   **AI 語義判斷批改**：這就是本專案的靈魂！我們不再使用簡單的「對/錯」按鈕。當您輸入答案後，程式會呼叫一個在您電腦上運行的**本地語言模型** (例如 `gemma:2b`) 來進行仿人類的批改。它能理解同義詞、近義詞甚至輕微的拼寫錯誤，讓學習過程更自然、更人性化。

-   **動態智慧抽考 (SRS)**：採用間隔重複系統 (Spaced Repetition System) 的核心思想。答對的單字會降低出現機率，答錯的單字則會被更頻繁地提問，集中火力解決您的學習弱點。

-   **完全本地運行，保障隱私**：從詞頻分析到 AI 模型判斷，所有操作都在您的個人電腦上完成。您的學習資料和進度完全私密，無需聯網，也無需擔心資料外洩。

-   **高度客製化的學習目標**：您只需準備一個簡單的 `exam_list.csv` 檔案，即可開始針對任何考試或課程進行最高效率的衝刺學習。

## 技術規格

-   **主要程式語言與版本**: Python 3.10
-   **核心函式庫**: Pandas, NLTK, Gensim, Requests
-   **本機 AI 引擎**: Ollama
-   **使用之本地語言模型**: Google Gemma (`gemma:2b`)
-   **詞頻分析資料來源**: 阿拉伯語維基百科 (Wikipedia Dump)
-   **開發環境**: Ubuntu 24.04 LTS

## 如何使用

請依照以下步驟設定並啟動您的 Vocabulary Speedruner。

### 步驟一：環境設定

1.  **確認 Python 版本**：確保您的電腦中安裝了 Python 3.10 或以上版本。
2.  **安裝必要函式庫**：打開終端機 (Terminal / PowerShell)，執行以下指令：
    ```bash
    pip install pandas requests nltk gensim
    ```
    *如果您在執行詞頻分析腳本時已安裝過 NLTK 和 Gensim，則無需重複安裝。*

### 步驟二：準備 AI 引擎 (Ollama)

1.  **安裝 Ollama**：前往 [Ollama 官網](https://ollama.com) 下載並安裝適合您作業系統的程式。
2.  **下載 AI 模型**：在終端機中執行以下指令來下載我們推薦的輕量級模型 `gemma:2b`。首次執行會需要一些時間下載（約 1.4 GB）。
    ```bash
    ollama run gemma:2b
    ```
    *下載完成後，您可以輸入 `/bye` 退出 Ollama 的對話模式。請確保 Ollama 服務在您學習時於背景運行。*

### 步驟三：準備單字表

請確保您的專案資料夾中包含以下 **3 個檔案**：

1.  **`voc_database.csv`**: 我們之前從維基百科分析出的主詞頻資料庫。
2.  **`exam_list.csv`**: 您本次需要學習的單字列表。請確保其格式如下 (第一行為標頭，且檔案需為 **UTF-8** 編碼)：
    ```csv
    arabic_word,translation
    كتاب,書本
    سيارة,汽車
    بيت,房子
    ```
3.  **`main_quiz.py`**: 我們的 AI 抽考主程式。

### 步驟四：開始學習！

一切準備就緒！在終端機中，執行以下指令啟動程式：

```bash
python main_quiz.py
# 如果您需要指定 Python 3.10 版本
# py -3.10 main_quiz.py